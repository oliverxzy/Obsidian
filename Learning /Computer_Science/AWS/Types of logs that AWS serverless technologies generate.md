## Types of logs that AWS serverless technologies generate
### AWS lambda logs
-   **function logs** (log written by a function, like print statement)
-   **extension logs** (emitted by lambda extension's code)
-   **platform logs** (generated by the lambda **runtime** and record invocation and extension-related events)
-   Each time you create a new function, CloudWatch Logs generates a new log group and log stream.

### Amazon API Gateway logs
(allow create APIs that act as the front door to backend services, such as lambda functions)
-   **execution logs**
    -   its document steps API Gateway takes to process a request. you can see details of requests to APIs along with the responses from integration backends and lambda authorizers.
-   **access logs**
    -   identify who accessed your API (e.g., source IP, user) and how they accessed it (e.g., HTTP method, resource path).

### Amazon DynamoDB logs
-   DynamoDB is a popular key-value and document database that provides low-latency data access at scale.
    -   capture table modifications in a stream
    -   it integrate with AWS CloudTrail to captures API call to and from DynamoDB and sends them as logs to S3 bucket

## AWS serverless logging Idea
-   you should have your application write every log in a structured format like JSON, which is both more human- and machine-readable.
-   Logging in JSON format also ensures that multi-line logs are processed as a single CloudWatch Logs event, which helps you avoid having related information distributed across multiple events.
-   you can log as levels to categorize how important a particular log message is

### Include useful information in logs
#### Requests
-   `requestTime` : The timestamp of the request
-   `requestId` : The API request ID given by API Gateway
-   `httpMethod` : The HTTP method used (e.g., DELETE, GET, POST, PUT)
-   `resourcePath` : The path to your resource (e.g.,  `/root/child` )
-   `status` : The status code of the response
-   `responseLatency` : The amount of time API Gateway took to respond to the request (in milliseconds)
#### Lambda authorizers
-   `authorizer.requestId` : The Lambda invocation request ID
-   `authorizer.status` : The status code returned by an authorizer, which indicates whether the authorizer responded successfully
-   `authorize.status` : The status code returned from an authorization attempt, which indicates whether the authorizer allowed or denied the request
-   `authorizer.latency` : The amount of time the authorizer took (in milliseconds) to run
-   `identity.user` : The principal identifier of the IAM user that made the request
-   `identity.sourceIP` : The source IP address of the TCP connection making the request to API Gateway endpoint
#### Integrations
-   `integration.requestId` : The integration’s request ID
-   `integration.status` : The status code returned by the integration’s code
-   `integration.integrationStatus` : The status code returned by the integration service
-   `integration.error` : The error message returned by the integration
-   `integration.latency` : The amount of time the integration took (in milliseconds) to run

## centralize logs with Datadog
You can send Lambda logs directly to Datadog—without having to forward them from CloudWatch Logs—by deploying the [Datadog Lambda extension](https://docs.datadoghq.com/serverless/libraries_integrations/extension/) as a Lambda Layer across all of your Python and Node.js functions. To submit logs from your Lambda integrations to Datadog, you’ll need to install the Datadog Forwarder Lambda function and subscribe it to your CloudWatch Logs log groups, as detailed in our [documentation](https://docs.datadoghq.com/logs/guide/send-aws-services-logs-with-the-datadog-lambda-function/?tab=awsconsole).

Once you’ve configured Datadog to collect logs from your serverless environment, you can begin exploring and analyzing them in real time in the [Log Explorer](https://app.datadoghq.com/logs). Datadog’s built-in log processing pipelines automatically extract metadata from your logs and turn them into tags, which you can use to slice and dice your data.
